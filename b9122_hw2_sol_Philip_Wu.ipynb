{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf86b0f",
   "metadata": {},
   "source": [
    "## Question 1 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4b75a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=https://www.federalreserve.gov/newsevents/pressreleases.htm\n",
      "num. of URLs in stack: 0 \n",
      "Trying to access= https://www.federalreserve.gov/newsevents/pressreleases.htm\n",
      "\n",
      "\n",
      "Unable to access= https://www.linkedin.com/company/federal-reserve-board\n",
      "HTTP Error 999: Request denied\n",
      "Unable to access= https://www.linkedin.com/company/federal-reserve-board\n",
      "HTTP Error 999: Request denied\n",
      "\n",
      "\n",
      "num. of URLs seen = 23, and scanned = 1\n",
      "\n",
      "\n",
      "The following are the webpages that contain the word 'covid'\n",
      "https://www.federalreserve.gov/default.htm\n",
      "https://www.federalreserve.gov/feeds/feeds.htm\n",
      "https://www.federalreserve.gov/publications.htm\n",
      "https://www.federalreserve.gov/sitemap.htm\n",
      "https://www.federalreserve.gov/azindex.htm\n",
      "https://www.federalreserve.gov/faqs.htm\n",
      "https://www.federalreserve.gov/aboutthefed/procurement/about.htm\n",
      "https://www.federalreserve.gov/monetarypolicy/policytools.htm\n",
      "https://www.federalreserve.gov/monetarypolicy/review-of-monetary-policy-strategy-tools-and-communications.htm\n",
      "https://www.federalreserve.gov/supervisionreg.htm\n",
      "https://www.federalreserve.gov/supervisionreg/topics/topics.htm\n",
      "https://www.federalreserve.gov/financial-stability/types-of-financial-system-vulnerabilities-and-risks.htm\n",
      "https://www.federalreserve.gov/financial-stability/responding-to-financial-system-emergencies.htm\n",
      "https://www.federalreserve.gov/paymentsystems.htm\n",
      "https://www.federalreserve.gov/paymentsystems/fr-payments-study.htm\n",
      "https://www.federalreserve.gov/econres/feds/index.htm\n",
      "https://www.federalreserve.gov/econres/notes/feds-notes/default.htm\n",
      "https://www.federalreserve.gov/econres/ifdp/index.htm\n",
      "https://www.federalreserve.gov/consumerscommunities/rural-community-economic-development.htm\n",
      "https://www.federalreserve.gov/consumerscommunities/community-development-publications.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/2021-press.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/2020-press.htm\n",
      "https://oig.federalreserve.gov/\n",
      "https://www.usa.gov/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "seed_url = \"https://www.federalreserve.gov/newsevents/pressreleases.htm\"\n",
    "\n",
    "urls = [seed_url]\n",
    "seen = [seed_url]\n",
    "opened = []\n",
    "contain_covid = []\n",
    "\n",
    "print(\"Starting with url=\"+str(seed_url))\n",
    "while len(urls) > 0 and len(contain_covid) < 10:\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        print(\"Trying to access= \"+curr_url)\n",
    "        print(\"\\n\")\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(webpage)\n",
    "\n",
    "    for tag in soup.find_all('a', href = True):\n",
    "        childUrl = tag['href']\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        #Then we need to check whehter the page this childUrl opens contain the word \"covid\"\n",
    "        try:\n",
    "            req_child = urllib.request.Request(childUrl,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            page_toCheck = urllib.request.urlopen(req_child).read()\n",
    "            soup_toCheck = BeautifulSoup(page_toCheck)\n",
    "            if \"covid\" in soup_toCheck.get_text().lower() and childUrl not in contain_covid:\n",
    "                #print(\"WE FOUND THE PAGE THAT CONTAINS COVID !!!\")\n",
    "                contain_covid.append(childUrl)\n",
    "        except Exception as ex:\n",
    "            print(\"Unable to access= \"+childUrl)\n",
    "            print(ex)\n",
    "            continue\n",
    "        #If the childUrl is in the seed_url domain, we should append it in the urls and seen lists\n",
    "        if \"https://www.federalreserve.gov/newsevents/pressreleases\" in childUrl and childUrl not in seen:\n",
    "            #print(\"seed_url is in childUrl and we have not seen this childUrl\")\n",
    "            #print(\"*** urls.append and seen.append ***\")\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "        #else:\n",
    "            #print(\"####################\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"num. of URLs seen = %d, and scanned = %d\" % (len(seen), len(opened)))\n",
    "print(\"\\n\")\n",
    "print(\"The following are the webpages that contain the word 'covid'\")\n",
    "for links in contain_covid:\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaef887",
   "metadata": {},
   "source": [
    "## Question 2 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0924d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=https://www.sec.gov/news/pressreleases\n",
      "num. of URLs in stack: 0 \n",
      "Trying to access= https://www.sec.gov/news/pressreleases\n",
      "\n",
      "\n",
      "num. of URLs seen = 31, and scanned = 1\n",
      "\n",
      "\n",
      "The following are the webpages that contain the word 'charges'\n",
      "The No 1  URL is  https://www.sec.gov/news/press-release/2022-183  with the text  SEC Charges Kim Kardashian for Unlawfully Touting Crypto Security\n",
      "The No 2  URL is  https://www.sec.gov/news/press-release/2022-182  with the text  SEC Charges Eight in Scheme to Fraudulently Promote Securities Offerings\n",
      "The No 3  URL is  https://www.sec.gov/news/press-release/2022-181  with the text  SEC Charges Two Canadian Software Engineers with Insider Trading\n",
      "The No 4  URL is  https://www.sec.gov/news/press-release/2022-180  with the text  SEC Charges Audit Firm RSM and Three Senior-Level Employees with Failure to Properly Conduct Client Audits\n",
      "The No 5  URL is  https://www.sec.gov/news/press-release/2022-179  with the text  Barclays Agrees to a $361 Million Settlement to Resolve SEC Charges Relating to Over-Issuances of Securities\n",
      "The No 6  URL is  https://www.sec.gov/news/press-release/2022-178  with the text  SEC Charges Man for Defrauding Investors out of Millions of Dollars by Posing as Hedge Fund Billionaire\n",
      "The No 7  URL is  https://www.sec.gov/news/press-release/2022-176  with the text  Deloitte’s Chinese Affiliate to Pay $20 Million Penalty for Asking Audit Clients to Conduct Their Own Audit Work\n",
      "The No 8  URL is  https://www.sec.gov/news/press-release/2022-175  with the text  SEC Charges The Hydrogen Technology Corp. and its Former CEO for Market Manipulation of Crypto Asset Securities\n",
      "The No 9  URL is  https://www.sec.gov/news/press-release/2022-174  with the text  SEC Charges 16 Wall Street Firms with Widespread Recordkeeping Failures\n",
      "The No 10  URL is  https://www.sec.gov/news/press-release/2022-173  with the text  SEC Charges Oracle a Second Time for Violations of the Foreign Corrupt Practices Act\n",
      "The No 11  URL is  https://www.sec.gov/news/press-release/2022-172  with the text  SEC Charges Father-Son Duo and Associate in Market Manipulation Schemes Resulting in a New Jersey Deli with a $100 Million Valuation\n",
      "The No 12  URL is  https://www.sec.gov/news/press-release/2022-171  with the text  SEC Charges Compass Minerals for Misleading Investors about Its Operations at World’s Largest Underground Salt Mine \n",
      "The No 13  URL is  https://www.sec.gov/news/press-release/2022-170  with the text  Boeing to Pay $200 Million to Settle SEC Charges that it Misled Investors about the 737 MAX\n",
      "The No 14  URL is  https://www.sec.gov/news/press-release/2022-169  with the text  SEC Charges Cheetah Mobile’s CEO and its Former President with Insider Trading \n",
      "The No 15  URL is  https://www.sec.gov/news/press-release/2022-168  with the text  Morgan Stanley Smith Barney to Pay $35 Million for Extensive Failures to Safeguard Personal Information of Millions of Customers\n",
      "The No 16  URL is  https://www.sec.gov/news/press-release/2022-167  with the text  Sparkster to Pay $35 Million to Harmed Investor Fund for Unregistered Crypto Asset Offering\n",
      "The No 17  URL is  https://www.sec.gov/news/press-release/2022-164  with the text  SEC Charges Gol Intelligent Airlines, Brazil’s Second Largest Airline, with FCPA Violations\n",
      "The No 18  URL is  https://www.sec.gov/news/press-release/2022-163  with the text  SEC Charges Loop Capital Markets in First Action against Broker-Dealer for Violating Municipal Advisor Registration Rule\n",
      "The No 19  URL is  https://www.sec.gov/news/press-release/2022-161  with the text  SEC Charges Four Underwriters in First Actions Enforcing Municipal Bond Disclosure Law\n",
      "The No 20  URL is  https://www.sec.gov/news/press-release/2022-160  with the text  SEC Charges VMware with Misleading Investors by Obscuring Financial Performance\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "seed_url = \"https://www.sec.gov/news/pressreleases\"\n",
    "\n",
    "urls = [seed_url]\n",
    "seen = [seed_url]\n",
    "opened = []\n",
    "contain_charges = []\n",
    "url_text = []\n",
    "\n",
    "print(\"Starting with url=\"+str(seed_url))\n",
    "while len(urls) > 0 and len(contain_charges) < 20:\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        print(\"Trying to access= \"+curr_url)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(webpage)\n",
    "\n",
    "    for tag in soup.find_all('a', href = True):\n",
    "        #Stop the loop when we have collected 20 urls that contain the word \"charges\"\n",
    "        if len(contain_charges) == 20:\n",
    "            break\n",
    "        \n",
    "        childUrl = tag['href']\n",
    "        text = tag.text\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        \n",
    "        #We need to make sure the webpage is a press release\n",
    "        if 'https://www.sec.gov/news/press-release' in childUrl:\n",
    "            #Then we need to check whehter the page this childUrl opens contain the word \"charges\"\n",
    "            try:\n",
    "                req_child = urllib.request.Request(childUrl,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                page_toCheck = urllib.request.urlopen(req_child).read()\n",
    "                soup_toCheck = BeautifulSoup(page_toCheck)\n",
    "                if \"charges\" in soup_toCheck.get_text().lower() and childUrl not in contain_charges:\n",
    "                    #print(\"WE FOUND THE PAGE THAT CONTAINS CHARGES !!!\")\n",
    "                    contain_charges.append(childUrl)\n",
    "                    url_text.append(text)\n",
    "            except Exception as ex:\n",
    "                print(\"Unable to access= \"+childUrl)\n",
    "                print(ex)\n",
    "                continue\n",
    "\n",
    "        #If the childUrl is in the seed_url domain, we should append it in the urls and seen lists\n",
    "        if \"https://www.sec.gov/news/press\" in childUrl and childUrl not in seen:\n",
    "            #print(\"seed_url is in childUrl and we have not seen this childUrl\")\n",
    "            #print(\"*** urls.append and seen.append ***\")\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"num. of URLs seen = %d, and scanned = %d\" % (len(seen), len(opened)))\n",
    "print(\"\\n\")\n",
    "print(\"The following are the webpages that contain the word 'charges'\")\n",
    "for i in range(len(contain_charges)):\n",
    "    print(\"The No\" ,i+1, \" URL is \", contain_charges[i], \" with the text \", url_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630887a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
